{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ea1b25-9168-4bbe-b62e-39d447a8cb81",
   "metadata": {},
   "source": [
    "Q1. Difference Between Linear Regression and Logistic Regression\n",
    "\n",
    "Feature\tLinear Regression\tLogistic Regression\n",
    "Dependent Var.\tContinuous\tCategorical (binary or multinomial)\n",
    "Prediction\tContinuous value (e.g., price)\tProbability of belonging to a class\n",
    "Model Output\tLinear equation (y = mx + b)\tS-shaped logistic function (sigmoid)\n",
    "Use Case\tPredicting continuous outcomes\tClassifying data points\n",
    "\n",
    "drive_spreadsheet\n",
    "Export to Sheets\n",
    "Scenario for Logistic Regression:\n",
    "\n",
    "Imagine you want to predict whether a customer will churn (cancel their service) based on factors like age, income, and spending habits. Here, churn (yes/no) is a binary categorical variable, making logistic regression ideal.\n",
    "\n",
    "Q2. Cost Function and Optimization in Logistic Regression\n",
    "\n",
    "Cost Function: Logistic regression employs the binary cross-entropy loss function. It measures the average difference between the predicted probabilities and the actual labels (0 or 1). Minimizing this loss leads to a better model.\n",
    "\n",
    "Optimization: Gradient descent is a common optimization technique. It iteratively adjusts the model's coefficients (weights) in a direction that reduces the cost function, leading to improved predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. Regularization to Prevent Overfitting\n",
    "ANS:\n",
    "\n",
    "Regularization penalizes models with overly complex structures, helping to prevent overfitting to the training data. Here are two common regularization methods:\n",
    "\n",
    "L1 regularization (LASSO): Introduces sparsity by shrinking some coefficient values to zero, potentially leading to feature selection.\n",
    "L2 regularization (ridge regression): Shrinks all coefficient values towards zero, reducing the model's flexibility but generally improving generalization.\n",
    "\n",
    "\n",
    "\n",
    "Q4. ROC Curve for Model Evaluation\n",
    "ANS:\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of a model's performance for a binary classification task. It plots the true positive rate (TPR) (sensitivity) on the y-axis against the false positive rate (FPR) (1 - specificity) on the x-axis.\n",
    "\n",
    "A model with a higher AUC (Area Under the Curve) on the ROC curve generally performs better.\n",
    "ROC curves are valuable for comparing models, especially when dealing with imbalanced datasets.\n",
    "\n",
    "\n",
    "\n",
    "Q5. Feature Selection Techniques\n",
    "ANS:\n",
    "\n",
    "Feature selection helps identify the most relevant features for improving model performance and reducing computational costs. Here are common techniques:\n",
    "\n",
    "Filter methods: Rank features based on statistical measures like correlation with the target variable or information gain.\n",
    "Wrapper methods: Evaluate feature subsets using model performance metrics, iteratively adding or removing features.\n",
    "Embedded methods: Feature selection is integrated into the model's training process (e.g., LASSO regularization).\n",
    "These techniques can lead to a more interpretable model and potentially reduce overfitting.\n",
    "\n",
    "Q6. Handling Imbalanced Datasets in Logistic Regression\n",
    "ANS :\n",
    "\n",
    "Imbalanced datasets occur when one class has significantly fewer data points than the other. This can lead to models biased towards the majority class. Here are strategies to address this:\n",
    "\n",
    "Oversampling: Duplicate data points from the minority class to create a more balanced dataset.\n",
    "Undersampling: Randomly remove data points from the majority class to achieve balance.\n",
    "SMOTE (Synthetic Minority Oversampling Technique): Create synthetic data points for the minority class.\n",
    "Cost-sensitive learning: Assign higher costs to misclassifying the minority class during training.\n",
    "\n",
    "\n",
    "Q7. Common Issues and Challenges\n",
    "ANS:\n",
    "\n",
    "\n",
    "Multicollinearity: When independent variables are highly correlated, it can lead to unstable coefficient estimates and inaccurate predictions. Techniques like removing correlated features or using regularization can help.\n",
    "Class Imbalance: Discussed in Q6.\n",
    "High dimensionality: Too many features can make training computationally expensive and lead to overfitting. Feature selection can be helpful here.\n",
    "Data quality: Ensure your data is clean and free of errors (missing values, outliers) for better model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
